{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+ToazCD9oUgsBlss3VzYh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrJHSIM/PyTorch_Practice/blob/main/Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation with `torch.autograd`\n",
        "- 신경망 학습할 때, 가장 자주 사용되는 알고리즘은 역전파(back propagation)임.\n",
        "- 매개변수의 손실함수의 미분에 따라 매개변수 가중치들이 조정됨.\n",
        "- PyTorch에서는 `torch.autograd` 엔진이 어떠한 계산 그래프도 자동으로 미분을 계산해줌."
      ],
      "metadata": {
        "id": "8s3M3BGWk2BQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rJAF-DtqkxuO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # 입력 텐서\n",
        "y = torch.zeros(3) # 기대되는 출력\n",
        "w = torch.randn(5, 3, requires_grad = True)\n",
        "b = torch.randn(3, requires_grad = True)\n",
        "z = torch.matmul(x, w) + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdoM2jsUXyOD"
      },
      "source": [
        "Tensors, Functions and Computational graph\n",
        "==========================================\n",
        "\n",
        "This code defines the following **computational graph**:\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/basics/comp-graph.png)\n",
        "\n",
        "In this network, `w` and `b` are **parameters**, which we need to\n",
        "optimize. Thus, we need to be able to compute the gradients of loss\n",
        "function with respect to those variables. In order to do that, we set\n",
        "the `requires_grad` property of those tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서를 만들 때 `requires_grad`의 값을 설정할 수 있지만, 나중에 `x.requires_grad_(True)` 메소드로도 가능함."
      ],
      "metadata": {
        "id": "2-O23hbbnMhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(5)  # 입력 텐서\n",
        "y = torch.zeros(3) # 기대되는 출력\n",
        "w = torch.randn(5, 3)\n",
        "b = torch.randn(3)\n",
        "w = w.requires_grad_(True)\n",
        "b = b.requires_grad_(True)\n",
        "z = torch.matmul(x, w) + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ],
      "metadata": {
        "id": "S9Y12kRsm7E1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Gradients\n",
        "- 신경망에서 매개변수들의 가중치들을 최적화 하기 위해, 매개변수에 관한 손실함수의 미분을 계산할 필요가 있음. ($\\frac{\\partial loss}{\\partial w}$와 $\\frac{\\partial loss}{\\partial b}$)\n",
        "- 이런 미분들을 계산하기 위해, `loss.backward()`을 불러오고 `w.grad`와 `b.grad`로 값들을 찾을 수 있음."
      ],
      "metadata": {
        "id": "92csQGZvnhKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLTw43ddoKCh",
        "outputId": "11ea3796-bfa0-4aa4-cf21-0d077691716d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2818, 0.0147, 0.1841],\n",
            "        [0.2818, 0.0147, 0.1841],\n",
            "        [0.2818, 0.0147, 0.1841],\n",
            "        [0.2818, 0.0147, 0.1841],\n",
            "        [0.2818, 0.0147, 0.1841]])\n",
            "tensor([0.2818, 0.0147, 0.1841])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbKeW3NQotMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj5-Rg18XyOF"
      },
      "source": [
        "## Disabling Gradient Tracking (미분 추적을 할 수 없게 할 경우)\n",
        "\n",
        "- 기본값으로 모든 텐서는 `requires_grad = True`이어서 계산 history와 미분 계산을 지원함.\n",
        "- 하지만, 이것이 필요하지 않을 때가 있는 데 학습된 모델로 추론을 할 경우임.\n",
        "- 즉, 오직 *forward* 계산만 할 경우임.\n",
        "- 이럴 경우 계산 코드를 `torch.no_grad()` 블록으로 감싸주면 됨.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-0vfzFGXyOF",
        "outputId": "17598dd1-0f26-4e94-be87-110ff17b0437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwAcCFUkpar9",
        "outputId": "9f503a6f-ee87-4f77-fa58-b79df6e3b5e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCc-ikM4XyOG"
      },
      "source": [
        "같은 결과를 얻는 다른 방법으로는 `detach()` 메소드를 텐서에 사용하는 것임."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IjLb06sXyOG",
        "outputId": "68908333-b4c7-4b91-aa88-debadf2aebb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w) + b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd8nQd5aps5V",
        "outputId": "bc9439d9-b98e-45ce-aef5-7791a10483fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYVjpDKaXyOH"
      },
      "source": [
        "Further Reading\n",
        "===============\n",
        "\n",
        "-   [Autograd\n",
        "    Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)\n"
      ]
    }
  ]
}