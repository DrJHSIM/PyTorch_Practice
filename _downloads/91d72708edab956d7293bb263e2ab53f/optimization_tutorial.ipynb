{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizng Model Parameters\n",
        "- 모델을 학습하는 것은 반복적인 과정임.\n",
        "- 경사 하강법(gradient descent)을 사용하여 변수들을 최적화(optimizes)시킴."
      ],
      "metadata": {
        "id": "ckziM6pGvJJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4NdURbyRvIFZ",
        "outputId": "2cdd0fec-fa3d-4bab-a8ae-22d4528d80c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16614164.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 306170.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5495089.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 17540436.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle = True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle = True)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 초매개변수들 (Hyperparameters)"
      ],
      "metadata": {
        "id": "2HQLQI3SwOaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 기계학습이 조정하는 매개변수 이외의 변수들을 초매개변수라함.\n",
        "- 다른 초매개변수들은 모델 학습에 영향을 줄 수 있음.\n",
        "- Epochs, Batch Size, Learning Rate 등이 있음.\n"
      ],
      "metadata": {
        "id": "1-G7BK8RwaS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "uljg-BG0ycz1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization Loop\n",
        "- 각 epoch의 주요 두 개의 부분으로 구성됨\n",
        "    : The Train Loop - 학습 데이터를 반복하고 매개변수들을 최적화함.\n",
        "    : The Validation/Test Loop - 모델 성능이 향상되었는 지 확인하기 위해 테스트 데이터를 반복함.\n",
        "\n",
        "## Loss Function\n",
        "- 모델에서 출력값과 정답값의 불일치 정도를 측정함.\n",
        "- 학습 동안 loss function을 최소화하는 것을 원함.\n",
        "- 회귀에서는 nn.MSELoss (Mean Square Error), 분류에서는 nn.NLLLoss(Negative Log Likelihood), nn.CrossEntropyLoss (nn.LogSoftmax + nn.NLLLoss)"
      ],
      "metadata": {
        "id": "Plm9HR9py4Wn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Io-jm21KvIFa"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tUQwBVxvIFb"
      },
      "source": [
        "Optimizer\n",
        "=========\n",
        "\n",
        "Optimization is the process of adjusting model parameters to reduce\n",
        "model error in each training step. **Optimization algorithms** define\n",
        "how this process is performed (in this example we use Stochastic\n",
        "Gradient Descent). All optimization logic is encapsulated in the\n",
        "`optimizer` object. Here, we use the SGD optimizer; additionally, there\n",
        "are many [different\n",
        "optimizers](https://pytorch.org/docs/stable/optim.html) available in\n",
        "PyTorch such as ADAM and RMSProp, that work better for different kinds\n",
        "of models and data.\n",
        "\n",
        "We initialize the optimizer by registering the model\\'s parameters that\n",
        "need to be trained, and passing in the learning rate hyperparameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6ByNOFMvIFb"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42Vemt9tvIFb"
      },
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        "\n",
        ":   -   Call `optimizer.zero_grad()` to reset the gradients of model\n",
        "        parameters. Gradients by default add up; to prevent\n",
        "        double-counting, we explicitly zero them at each iteration.\n",
        "    -   Backpropagate the prediction loss with a call to\n",
        "        `loss.backward()`. PyTorch deposits the gradients of the loss\n",
        "        w.r.t. each parameter.\n",
        "    -   Once we have our gradients, we call `optimizer.step()` to adjust\n",
        "        the parameters by the gradients collected in the backward pass.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "- Optimization(최적화)는 각 학습 단계에서 모델의 에러를 줄이기 위해 모델 매개변수들을 조정하는 과정임.\n",
        "- Optimization algorithms은 어떻게 이 과정을 수행하는지 정의한 것임.\n",
        "- 모든 optimization logicsms `optimizer` 객체 안에 캡슐화됨.\n",
        "- 여기서는 SGD optimizer를 사용함. (다른 많은 optimizer은 [different optimizers](https://pytorch.org/docs/stable/optim.html)찾아볼 수 있음)"
      ],
      "metadata": {
        "id": "37Rn-EyH1i-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "ax8faDxS29O7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습하는 loop안에서, 최적화되는 3단계를 겪음.\n",
        "1. `optimizer.zero_grad()` : 모델의 매개변수의 기울기를 초기화함. 기울기는 기본적으로 추가하게 되어 있음. 따라서 double-counting을 막기 위해서는 각 반복마다 명확히 기울기를 zero로 만들어 줘야함.\n",
        "2. 역전파(Backpropagate)는 `loss.backward()`를 불러옴. (loss의 기울기를 축적함)\n",
        "3. 일단 기울기를 가지고 있으면, `optimizer.step()`을 불러와 역방향으로 통과하며 축적된 기울기에 의해 매개변수들이 조정됨.\n"
      ],
      "metadata": {
        "id": "eGEeOH8G3FXB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjIeTXGxvIFb"
      },
      "source": [
        "## Full Implementation\n",
        "\n",
        "We define `train_loop` that loops over our optimization code, and\n",
        "`test_loop` that evaluates the model\\'s performance against our test\n",
        "data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M82G79tLvIFb"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader)"
      ],
      "metadata": {
        "id": "NzitCKwa8o8_",
        "outputId": "f7791a23-3856-40d6-e623-23a39e4fee90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "938"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, optimizer, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        preds = model(x)\n",
        "        loss = loss_fn(preds, y)\n",
        "\n",
        "        #backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch+1)*batch_size\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    len_batch = len(dataloader)\n",
        "    total_loss, correct = 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            preds = model(x)\n",
        "            total_loss += loss_fn(preds, y).item()\n",
        "            correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss = total_loss/len_batch # average for batch_size\n",
        "    test_correct = correct/len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*test_correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "SLzng-Zt6-Wq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- loss function과 optimizer를 초기화 하고 이것을 train_loop와 test_loop에 통과시킴.\n",
        "- epochs의 수를 증가시켜 모델의 성능 향상을 확인."
      ],
      "metadata": {
        "id": "d7tpQkM_BJJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n----------------------------\")\n",
        "    train_loop(train_dataloader, model, optimizer, loss_fn)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "_tMdz_w_BIL9",
        "outputId": "7990d665-ab90-4f1a-e974-27824eae75d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "----------------------------\n",
            "loss: 2.313744  [   64/60000]\n",
            "loss: 2.297681  [ 6464/60000]\n",
            "loss: 2.280901  [12864/60000]\n",
            "loss: 2.259946  [19264/60000]\n",
            "loss: 2.247251  [25664/60000]\n",
            "loss: 2.247690  [32064/60000]\n",
            "loss: 2.217069  [38464/60000]\n",
            "loss: 2.211756  [44864/60000]\n",
            "loss: 2.208843  [51264/60000]\n",
            "loss: 2.169601  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.8%, Avg loss: 2.161524 \n",
            "\n",
            "Epoch 2\n",
            "----------------------------\n",
            "loss: 2.152873  [   64/60000]\n",
            "loss: 2.139109  [ 6464/60000]\n",
            "loss: 2.133725  [12864/60000]\n",
            "loss: 2.085512  [19264/60000]\n",
            "loss: 2.048757  [25664/60000]\n",
            "loss: 2.032989  [32064/60000]\n",
            "loss: 2.014278  [38464/60000]\n",
            "loss: 1.996171  [44864/60000]\n",
            "loss: 1.953714  [51264/60000]\n",
            "loss: 1.919172  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.7%, Avg loss: 1.908196 \n",
            "\n",
            "Epoch 3\n",
            "----------------------------\n",
            "loss: 1.885417  [   64/60000]\n",
            "loss: 1.905309  [ 6464/60000]\n",
            "loss: 1.854254  [12864/60000]\n",
            "loss: 1.756485  [19264/60000]\n",
            "loss: 1.743126  [25664/60000]\n",
            "loss: 1.707375  [32064/60000]\n",
            "loss: 1.675958  [38464/60000]\n",
            "loss: 1.659525  [44864/60000]\n",
            "loss: 1.584067  [51264/60000]\n",
            "loss: 1.488163  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.7%, Avg loss: 1.531764 \n",
            "\n",
            "Epoch 4\n",
            "----------------------------\n",
            "loss: 1.464086  [   64/60000]\n",
            "loss: 1.501877  [ 6464/60000]\n",
            "loss: 1.453161  [12864/60000]\n",
            "loss: 1.419364  [19264/60000]\n",
            "loss: 1.470060  [25664/60000]\n",
            "loss: 1.419275  [32064/60000]\n",
            "loss: 1.168367  [38464/60000]\n",
            "loss: 1.385268  [44864/60000]\n",
            "loss: 1.185279  [51264/60000]\n",
            "loss: 1.334106  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 1.256847 \n",
            "\n",
            "Epoch 5\n",
            "----------------------------\n",
            "loss: 1.260473  [   64/60000]\n",
            "loss: 1.296507  [ 6464/60000]\n",
            "loss: 1.226209  [12864/60000]\n",
            "loss: 1.252286  [19264/60000]\n",
            "loss: 1.111418  [25664/60000]\n",
            "loss: 1.189925  [32064/60000]\n",
            "loss: 1.090692  [38464/60000]\n",
            "loss: 1.084394  [44864/60000]\n",
            "loss: 1.105837  [51264/60000]\n",
            "loss: 1.114025  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.2%, Avg loss: 1.092474 \n",
            "\n",
            "Epoch 6\n",
            "----------------------------\n",
            "loss: 1.134445  [   64/60000]\n",
            "loss: 0.971029  [ 6464/60000]\n",
            "loss: 1.126796  [12864/60000]\n",
            "loss: 1.065025  [19264/60000]\n",
            "loss: 0.958178  [25664/60000]\n",
            "loss: 1.128764  [32064/60000]\n",
            "loss: 1.052809  [38464/60000]\n",
            "loss: 0.985198  [44864/60000]\n",
            "loss: 0.840091  [51264/60000]\n",
            "loss: 0.951781  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.8%, Avg loss: 0.986812 \n",
            "\n",
            "Epoch 7\n",
            "----------------------------\n",
            "loss: 0.844383  [   64/60000]\n",
            "loss: 0.951480  [ 6464/60000]\n",
            "loss: 0.931791  [12864/60000]\n",
            "loss: 0.883089  [19264/60000]\n",
            "loss: 0.864256  [25664/60000]\n",
            "loss: 0.988424  [32064/60000]\n",
            "loss: 0.781019  [38464/60000]\n",
            "loss: 0.849322  [44864/60000]\n",
            "loss: 0.825348  [51264/60000]\n",
            "loss: 0.941638  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.9%, Avg loss: 0.914722 \n",
            "\n",
            "Epoch 8\n",
            "----------------------------\n",
            "loss: 0.734491  [   64/60000]\n",
            "loss: 0.977797  [ 6464/60000]\n",
            "loss: 0.856693  [12864/60000]\n",
            "loss: 0.902784  [19264/60000]\n",
            "loss: 0.776189  [25664/60000]\n",
            "loss: 0.802160  [32064/60000]\n",
            "loss: 0.979177  [38464/60000]\n",
            "loss: 0.845964  [44864/60000]\n",
            "loss: 0.778721  [51264/60000]\n",
            "loss: 0.751552  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.5%, Avg loss: 0.859652 \n",
            "\n",
            "Epoch 9\n",
            "----------------------------\n",
            "loss: 0.825282  [   64/60000]\n",
            "loss: 0.778849  [ 6464/60000]\n",
            "loss: 0.952999  [12864/60000]\n",
            "loss: 0.753118  [19264/60000]\n",
            "loss: 0.857719  [25664/60000]\n",
            "loss: 0.860212  [32064/60000]\n",
            "loss: 0.808852  [38464/60000]\n",
            "loss: 0.743781  [44864/60000]\n",
            "loss: 0.799916  [51264/60000]\n",
            "loss: 0.848691  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.8%, Avg loss: 0.822594 \n",
            "\n",
            "Epoch 10\n",
            "----------------------------\n",
            "loss: 0.582946  [   64/60000]\n",
            "loss: 0.744442  [ 6464/60000]\n",
            "loss: 0.906656  [12864/60000]\n",
            "loss: 0.848121  [19264/60000]\n",
            "loss: 0.907560  [25664/60000]\n",
            "loss: 0.660994  [32064/60000]\n",
            "loss: 0.732279  [38464/60000]\n",
            "loss: 0.757505  [44864/60000]\n",
            "loss: 0.623808  [51264/60000]\n",
            "loss: 0.812017  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 0.787673 \n",
            "\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDfyyjycvIFc"
      },
      "source": [
        "Further Reading\n",
        "===============\n",
        "\n",
        "-   [Loss\n",
        "    Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "-   [torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
        "-   [Warmstart Training a\n",
        "    Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}